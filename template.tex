\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{epsfig,endnotes}
\usepackage{times}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage[usenames]{color}
\usepackage{verbatim}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\topmargin -0.75in
\columnsep 0.25in


\begin{document}


\title{\Large Applications of weakly consistent distributed data structures over shared logs}

\author{Pranav Maddi}

\date{}
\maketitle

\tableofcontents

\subsection*{Abstract}

I describe two possible extensions to the SDN framework that could benefit the application of network virtualization.

\section{Background}

\subsection{Shared logs}

A set of totally ordered shared logs can provide weaker consistency on the data structure built on top of it. Each element in the logs have a set of dependencies. That is, to know element \verb_e_, all of the elements of \verb_e.dependencies_ must be loaded.  By allowing dependencies to be specified between elements of various shared logs, arbitrary partial orderings of nodes can be constructed while preserving the convenience of having totally ordered logs. By carefully selecting what operations on the higher level data structure require global reads and writes, causal ordering can be guaranteed across the network.

Tango \cite{Tango} aims to provide a way to store metadata by allowing transactional access to arbitrary data structures with persistence and high availability. The Tango paper notes that some services such as ZooKeeper provide persistence, high availability, and strong consistency---but only for specific data structures.

Tango objects are a class of data structures stored in memory that built on the shared log. The history of the object is stored directly in the log, and each object can have multiple views which are copies of the data structure stored by the clients. The source of truth in the system is the shared log, and the views are ``soft state'' that can be reconstructed by replaying the shared history. Modifications to Tango objects are made by appending updates to the history, and accesses are made by syncing the view with the history.

Using a shared log makes strongly consistent operations easier than they would be in a conventional distributed system. Such applications include remote mirroring, coordinated rollbacks, and consistent snapshots across objects. These options are available since views can be synced to any offset in the log and replayed as needed.

The shared log abstraction represents a total ordering of the updates added to the log. This abstraction is a common way to organize changes to a file system, for instance \cite{Log}. By definition, this means that it is a set with a relation that satisfies reflexivity, transitivity and symmetry, where any two items can be compared. On the other hand, we want to represent a partial ordering of the updates so that we can allow the user the flexibility of not relying on other users writes. A directed acyclic graph is a data structure that can represent partial ordering relations. Initially, we considered using a shared DAG as a representation of state in the system. However, this seemed to leave the system too unconstrained since it would be difficult to maintain ordering across nodes and read forward to get new data. 

\subsection{Consistency}

Not all applications require a strong consistency guarantee. Depending on the particular application, lower latency reads or the ability to write immediately could be preferred. We would like to enable our users to select what level of consistency works for them. Rather than have the consistency level be specified at development time, we would like to support this at the API level. By having something like a Service Level Agreement specified within the update\_helper and query\_helper, the client could configure the guarantee that works best in the situation. The SLA specified would then inform where in the DAG to place the update. The motivation for such a system is that different procedures in an application can have different guarantee requirements.

SLA requirements we may support include: \cite{SLA}

\begin{description}
\item[strong] returns the value of the last preceding Put(key) performed by any client
\item[read-my-writes] returns the value written by the last preceding Put(key) in the same session or returns a later version; if no Puts have been performed to this key in this session, then the Get may return any previous value as in eventual consistency.
\item[bounded(t)] returns a value that is stale by at most t seconds. Specifically, it returns the value of the latest Put(key) that completed more than t seconds ago or some more recent version.
\item[causal] returns the value of a latest Put(key) that causally precedes it or returns some later version. The causal precedence relation $<$ is defined such that op1 $<$ op2 if either
(a) op1 occurs before op2 in the same session,
(b) op1 is a Put(key) and op2 is a Get(key) that
returns the version put in op1, or
(c) for some op3, op1 $<$ op3 and op3 $<$ op2
\end{description}

Any of these consistencies could be supported with the multiple shared log structure, as long we carefully implement the distributed data structure on top of it.

\section{Software Defined Networking}

In general, SDN allows central control of a network. It aims to keep the complexity of the network manageable by providing abstractions for the underlying network devices. SDN provides four main advantages over traditional networks:\cite{jain2013network}

\begin{itemize}
\item Separation of the control and data planes
\item Centralization of the control plane
\item Programmability of the control plane
\item Standardization of API's
\end{itemize}
\cite{SDN}

Networking protocols are often arranged into data and control planes. The data plane consists of all the messages that are generated by the network clients. The network must then decide how to transport the packets from source to destination, which requires some level of decision making by the control layer. In a traditional network, these decisions are made by the control layer of the router which determines which routing behavior to perform. This might include computing L3 or L2
routing protocols such as Open Shortest Path First or Spanning Tree. A network manager might keep track of traffic statistics and the network state.

With SDN, this control logic is both separate and centralized, away from the data plane. The data plane still forwards the packets using the forwarding table assigned by the control plane, but that logic is separated out into a controller. This has the added benefit of reducing the cost of the switches. SDN's centralization of control makes determining the network state faster than with standard distributed network practices, and policy changes can be propagated much faster.
Centralized control in SDN networks also have scaling issues, and it is necessary to divide the network into subnets that can share control strategy. OpenFlow is the main southbound protocol for controllers to interact with routers and switches. It gives the controller access to the forwarding plane of the network, and provides the ability to add, modify, and remove packet matching rules and actions.\cite{mckeown2008openflow}

% \subsection*{OpenFlow}
% 
% OpenFlow is the protocol for communicating with the routers and switches. It allows the controller to install flows in the system.
% 
\subsection{ONOS}

ONOS is an open source network ``operating system'' that aims to have distributed control of the network. Like the other systems covered here, ONOS uses the OpenFlow control-data plane interface. The overall architecture of ONOS is composed of a network view that contains a network graph abstraction, OpenFlow managers, as well as a Zookeeper registry for coordination. The main goal of ONOS is to provide effective network virtualization.\cite{berde2014onos}

To interact with the OpenFlow interface, ONOS uses modules from Floodlight, which is another SDN implementation. This includes the switch manager, the IO loop, and link discovery. These modules as well as shared Floodlight module manager means that ONOS is slightly more compatible with other SDN controllers.

The network view data model was stored in an ONOS graph database built on top of a distributed and persistent key-value store called \emph{RamCloud}. RamCloud exposes the Blueprint graph API which applications use to access the network state. The system guarantees eventual consistency in the graph view.\cite{berde2014onos} 

The transition semantics of the graph database are maintained to correspond with structural nature of the graph, such as requiring that an edge connects two nodes. To justify that eventual consistency is good enough for the graph view, some of the ONOS experiments show that when using redundant and identical flow path computation on all instances, the computation modules on each instance converge to the same path. Also, legacy networks have embedded control and are not logically centralized, so it's eventual consistency doesn't impede it's normal operations. However, the authors of the paper admit that there could be some benefit to having some sequencing guarantees that provide deterministic state transition in the network.

Out of the box, ONOS has support for a distributed controller architecture. It can run on multiple servers, where each is the master OpenFlow controller for some switches, and any switch has exactly one master controller. Therefore, an ONOS instance must propagate state changes from the switches in its domain to the network of ONOS controllers. The system also aims to be scalable so that as data plane capacity grows or control plane demand increases, more ONOS instances can be created.

ONOS uses Zookeeper as a coordination mechanism for general fault tolerance and master election for switches. Since ONOS is distributed, it has secondary controllers for each switch in case the primary controller goes down. That way, the load of a primary failure are distributed to the remaining controllers. During runtime failure of any nodes detected by the lack of a keep-alive message, a replacement leader or follower is elected to replace it.

Since ONOS assigns a switch to multiple instances, and exactly one master, it also uses a master election process for failure handling. The switch's master connection communicates all the relevant information about the switch to the network view, and the secondary controllers for the switch are only required to elect a new master. The authors emphasize that using Zookeeper to coordinate this election ensures that switches always have a unique master. 

The data model behind the graph abstraction represents each network object in a different table. This was implemented as a means to reduce the number of updates, since a previous version stored all the network objects in a single table. 

% (We could store elements of the network in separate logs, since they don't have dependencies among each other) 
% (We could potentially allow for causal consistency in the graph view with our logs. Whether this is actually useful is questionable though.)
% (An issue with using a fuzzy log in this system to represent network state is that polling is inefficient. The authors found that they needed to build an event notification system so that the central database wouldn't need to be polled for changes. In a fuzzy log situation, reads require going to the central database, so it may be less efficient.)

<onos diagram>

The Network View API

% \subsection{Ryu}
% 
% Dec 2
% 
\subsection{OpenDaylight}

The OpenDaylight defines a model driven approach for managing software defined networks. It aims to create a more modular system for SDN, where subscribers and providers for services are plugins to the system. This Model Driven Software Engineering approach creates a framework for relationships between models, standard mappings and patterns that enable model generation. An advantage of the system, therefore, is that it can create code and API's directly from models.\cite{ODL}

OpenDaylight controller uses YANG as a data modeling language to specify these cross platform portable data types, which can describe network operation. While YANG was initially developed for this purpose, it effectively describes other network elements, including policy, protocol, and subscriptions to services. Rather than being an object-oriented structure, YANG is tree-structured and includes complex data types like lists and unions.\cite{yang}

For example to represent a grouping of nodes in YANG:\cite{yang}
\begin{lstlisting}
grouping target {
    leaf address {
        type inet:ip-address;
        description "Target IP address";
    }
    leaf port {
        type inet:port-number;
        description "Target port number";
    }
}
container peer {
    container destination {
        uses target;
    }
}
\end{lstlisting}

To modify the data defined by YANG, OpenDaylight uses the NETCONF and RESTCONF protocols. NETCONF is a network management protocol that defines the configuration and operations on a data store. It also has the ability to run remote procedure calls and send notifications about the underlying data. NETCONF encodes its messages and data in XML. RESTCONF is similar to NETCONF, except that it exports a REST interface over HTTP for use by external applications.\cite{enns2011netconf}

An operation configuring such a peer with NETFCONF:\cite{yang}
\begin{lstlisting}
<peer>
  <destination>
    <address>192.0.2.1</address>
    <port>830</port>
  </destination>
</peer>
\end{lstlisting}


We will focus on the specific application of network virtualization in OpenDaylight. The network virtualization libarary of OpenDaylight comes packaged with the ODL Controller Platform and provides the functionality to run multiple controller nodes in a logically centralized manner \cite{ONV:wiki}. Just as with ONOS, each switch in the system is assigned a leader controller.This virtualization services allow machines to be grouped into virtual network segments (VNS) which can be used to logically segment a network acorrding to a specialized use case. Some advantages of doing this include sharing of resources, isolation, agregration of resources, and increased ease of management \cite{jain2013network}.

Of the network functions served by the ODL controller, which include a statistics manager, forwarding rules manager, and an inventory manager, the topology service is of particular importance for network virtualization becuase it is required to translate virtual routing descisions to network flows \cite{jain2013network}. To achieve this, the system stores the messages from BGP-LS and passes them to the Topology Exporter that generates that makes available the topology of the controller's switchces via RESTCONF. The topology includes the connections between nodes, routers, switches, hosts and various appliances of the netowork. This system produces a reliable network view in the case of a single controller, but becomes more complicated if more than one controller is used, since the sub-network views must be constructed into the global view to make flow descisions \cite{bgp:rfc}.


% 
% Datastore can be pugged into any implementation
% Can get topology information from a bgp feed
% 
% Plugins?
% Applications:
% 	Bgp and topology
% 	Model to model
% 	Generic model adoption
% 
% 
% OpenDaylight network virtualization


% <The OpenDaylight controller is built using the Java OSGi framework. OSGi adds a modular framework that allows starting, stopping, loading and unloading of Java modules without bringing down the entire running JVM platform. Ryu controller does not offer this comodity. In Ryu we need to stop the controller and run it again with the needed modules for the REST methods we want to execute or we need to build a REST API with all REST modules included in it, which runs once and provides all REST methods without stopping the controller. >

% how is topology discovered


% \section{Common Data Structures}

% ugh

\section{Proposed SDN extensions}

There are two main representations of state in the systems we have discussed. There is a coordination service ONOS that maintains the naming and other information that must be stored by the controllers. Also, there is a representaion of the graph topology that each controller must learn from the physical switches under its domain. Then, to generate the full topology of the network, these subnetwork topologies must be assembled in the full network view.

\subsection{Georeplication}

In both ONOS, the graph topology information stored by the controllers is eventually consistent. The ONOS paper suggests that this is "good enough" for traditional netowrking, and presumably good enough for their applications \cite{berde2014onos}. 

Similarly, with OpenDaylight, the graph topology of each controller is generated by BGP and disseminated to the remaining controllers. To generate the full nework view, these sub-network topologies are passed to the master controller-node of the network \cite{ONV:wiki}. However, these network change events are assumed to be rare, and can only gurarantee eventual consistency over the topology link-state graph \cite{bgp:rfc}.

However, I would argue that this can lead to problems with advanced SDN applications, primarily with network virtualization, since effective netowork virtualzation must be able to reliably isolate resources.

Consider a network configuration with two groups that must be kept separate---Imagine that the engineering and accunting machines of an organization are on the same SDN network, and we want no packets to flow between the Engineering Virtual Network Segement and the Accounting VNS. In this case, imagine the following occurs:

\begin{description}
\item [Event 1] Alice connects to a controller in the Engineering VNS.
\item [Event 2] Alice connects to a controller in the Accounting VNS.
\item [Event 3] Alice disconnects from  the Engineering VNS controller.
\end{description}

Instead, with eventual consistency over the topology, the following ordering of events could be observed after those events:

\begin{description}
\item [Event 1] Alice connects to a controller in the Engineering VNS.
\item [Event 3] Alice disconnects from the Engineering VNS controller.
\item [Event 2] Alice connects to a controller in the Accounting VNS.
\end{description}

Which the virtualization service would deem valid after Event 3.

Therefore, before it is possble for the network virtualizaton application to observe after the disconnection event that Alice is disconnected from the Enginerring VNS. In reality, however, Alice is connected to both the Engineering and Accounting VNS's. Clearly, in this case, some security measures could be installed to prevent such attacks on the network such as requiring authentiction or ACL's---But it does illustrate that only eventual consistency leaves something to be desired when virtualzing a network. Such problems compound when we try to build larger scale geo-distrubuted virtualized networks, since correlated network splits and latency issues are compounded.

To enforce the global invariants required of the VNS isolaton, a simple solution is to simply require strong consistency over the link-state graph views \cite{bgp:rfc}. This could be implemented by having a Paxos-like service that requires majority consensus to make link-state changes. Then, installed isolation rules could be guaranteed over the network view. This is not an unreasonable consideration---ONOS already maintains global coordination state about the in
controller-switch relationships in ZooKeeper.
about the primarily about 

Could using a shared-log approach to causal ordering guarantees be useful? As 

\subsection{Coordination}

ONOS maintains a 

 Is this a global invariant that any switch cannot be part of the South and North Korea VNS's? If so, supporting causal consistency of the graph operations wouldn't do much to prevent such an attack on the system. 

 Different netowrk elements only work with their own data.

 The shared-log model of ZooKeeper could be very useful in the case of nework separation. If the durable store were maintained at each VNS, then inventory managment could be logically segmented into 

\subsection{ONOS}

make graph structure causally consistent (topology representations with a log backing...)

build a log based event notification system?

causal graph system
    eventual consistency does not seem to affect the control plane behavior in traditional applications

adjacency matrix of the graph in the logs

causal coordination system since there are diffrent network elements

maintian separations of the topology graph with adjancency in the runtime
\subsection{OpenDaylight}

OpenDaylight's modularity and data store agnostic setup makes it well suited for weakly consistent structures.

\section{Conclusion}

\bibliographystyle{acm}
\bibliography{template}

\end{document}
